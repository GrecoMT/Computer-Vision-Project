{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10272279,"sourceType":"datasetVersion","datasetId":6355820},{"sourceId":10314132,"sourceType":"datasetVersion","datasetId":6384630}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom collections import Counter\n\nlabels_path = \"/kaggle/input/dataset-soccer/train/labels\"\n\ndef calculate_label_balance(labels_path):\n    if not os.path.isdir(labels_path):\n        print(f\"La directory {labels_path} non esiste.\")\n        return\n\n    positive_labels = 0\n    negative_labels = 0\n\n    for label_file in os.listdir(labels_path):\n        if label_file.endswith(\".txt\"):\n            label_file_path = os.path.join(labels_path, label_file)\n            # Verifica se il file Ã¨ vuoto o contiene dati\n            if os.path.getsize(label_file_path) > 0:\n                positive_labels += 1\n            else:\n                negative_labels += 1\n\n    total_files = positive_labels + negative_labels\n\n    print(\"Bilanciamento delle etichette:\")\n    print(f\"File con etichette positive (non vuoti): {positive_labels} ({(positive_labels / total_files) * 100:.2f}%)\")\n    print(f\"File con etichette negative (vuoti): {negative_labels} ({(negative_labels / total_files) * 100:.2f}%)\")\n    print(f\"Totale file: {total_files}\")\n\ncalculate_label_balance(labels_path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:07:14.778179Z","iopub.execute_input":"2025-01-04T17:07:14.778647Z","iopub.status.idle":"2025-01-04T17:07:32.379205Z","shell.execute_reply.started":"2025-01-04T17:07:14.778611Z","shell.execute_reply":"2025-01-04T17:07:32.378078Z"}},"outputs":[{"name":"stdout","text":"Bilanciamento delle etichette:\nFile con etichette positive (non vuoti): 3742 (39.00%)\nFile con etichette negative (vuoti): 5854 (61.00%)\nTotale file: 9596\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\nimport shutil\n\ntrain_labels_path = \"/kaggle/input/dataset-soccer/train/labels\"\ntrain_images_path = \"/kaggle/input/dataset-soccer/train/images\"\nval_labels_path = \"/kaggle/input/dataset-soccer/val/labels\"\nval_images_path = \"/kaggle/input/dataset-soccer/val/images\"\noutput_labels_path = \"/kaggle/working/train_val/labels\"\noutput_images_path = \"/kaggle/working/train_val/images\"\n\nos.makedirs(output_labels_path, exist_ok=True)\nos.makedirs(output_images_path, exist_ok=True)\n\ndef merge_datasets(train_labels_path, train_images_path, val_labels_path, val_images_path, output_labels_path, output_images_path):\n    for label_file in os.listdir(train_labels_path):\n        shutil.copy(os.path.join(train_labels_path, label_file), os.path.join(output_labels_path, label_file))\n        image_file = label_file.replace(\".txt\", \".jpg\")\n        shutil.copy(os.path.join(train_images_path, image_file), os.path.join(output_images_path, image_file))\n\n    for label_file in os.listdir(val_labels_path):\n        shutil.copy(os.path.join(val_labels_path, label_file), os.path.join(output_labels_path, label_file))\n        image_file = label_file.replace(\".txt\", \".jpg\")\n        shutil.copy(os.path.join(val_images_path, image_file), os.path.join(output_images_path, image_file))\n\n    print(\"Merge completato: dati salvati in /kaggle/working/train_val.\")\n\n# Esegui la funzione\nmerge_datasets(train_labels_path, train_images_path, val_labels_path, val_images_path, output_labels_path, output_images_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:10:14.182125Z","iopub.execute_input":"2025-01-04T17:10:14.182465Z","iopub.status.idle":"2025-01-04T17:10:50.237769Z","shell.execute_reply.started":"2025-01-04T17:10:14.182436Z","shell.execute_reply":"2025-01-04T17:10:50.236469Z"}},"outputs":[{"name":"stdout","text":"Merge completato: dati salvati in /kaggle/working/train_val.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install -U albumentations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:11:00.260605Z","iopub.execute_input":"2025-01-04T17:11:00.260944Z","iopub.status.idle":"2025-01-04T17:11:04.478184Z","shell.execute_reply.started":"2025-01-04T17:11:00.260915Z","shell.execute_reply":"2025-01-04T17:11:04.476916Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.24)\nRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\nRequirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.23)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.23->albumentations) (3.11.3)\nRequirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.23->albumentations) (6.2.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (2.23.4)\nRequirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os\nimport random\nimport shutil\nimport albumentations as A\nfrom albumentations import GaussianBlur\nfrom albumentations.augmentations.transforms import RandomBrightnessContrast, HueSaturationValue, Sharpen, ISONoise\nfrom albumentations.core.composition import OneOf\nimport cv2\n\nlabels_path = \"/kaggle/working/train_val/labels\"\nimages_path = \"/kaggle/working/train_val/images\"\noutput_labels_path = \"/kaggle/working/train_val_edit/labels\"\noutput_images_path = \"/kaggle/working/train_val_edit/images\"\n\nos.makedirs(output_labels_path, exist_ok=True)\nos.makedirs(output_images_path, exist_ok=True)\n\ndef balance_dataset(labels_path, images_path, output_labels_path, output_images_path):\n    if not os.path.isdir(labels_path) or not os.path.isdir(images_path):\n        print(\"Una delle directory specificate non esiste.\")\n        return\n\n    positive_files = []\n    negative_files = []\n\n    for label_file in os.listdir(labels_path):\n        if label_file.endswith(\".txt\"):\n            label_file_path = os.path.join(labels_path, label_file)\n            if os.path.getsize(label_file_path) > 0:\n                positive_files.append(label_file)\n            else:\n                negative_files.append(label_file)\n\n    # Sottocampionamento dei negativi\n    sampled_negatives = random.sample(negative_files, len(positive_files))\n    print(f\"sample negatives:  {len(sampled_negatives)}\")\n\n    for label_file in sampled_negatives:\n        shutil.copy(os.path.join(labels_path, label_file), os.path.join(output_labels_path, label_file))\n        image_file = label_file.replace(\".txt\", \".jpg\")\n        shutil.copy(os.path.join(images_path, image_file), os.path.join(output_images_path, image_file))\n\n    # Sovracampionamento dei positivi con augmentation\n    augmentation = A.Compose([\n        OneOf([\n            RandomBrightnessContrast(p=0.5),\n            HueSaturationValue(p=0.5),\n            ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.5),\n            GaussianBlur(blur_limit=(3, 7), p=0.5),\n            Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.5)\n        ], p=1),\n    ])\n\n    for label_file in positive_files:\n        label_file_path = os.path.join(labels_path, label_file)\n        image_file = label_file.replace(\".txt\", \".jpg\")\n        image_file_path = os.path.join(images_path, image_file)\n\n        shutil.copy(label_file_path, os.path.join(output_labels_path, label_file))\n        shutil.copy(image_file_path, os.path.join(output_images_path, image_file))\n\n        # Augmentation\n        image = cv2.imread(image_file_path)\n        for i in range(2):  # Crea 2 immagini aumentate per ogni positivo\n            augmented = augmentation(image=image)\n            aug_image = augmented[\"image\"]\n\n            aug_image_name = f\"aug_{i}_{image_file}\"\n            cv2.imwrite(os.path.join(output_images_path, aug_image_name), aug_image)\n            with open(os.path.join(output_labels_path, f\"aug_{i}_{label_file}\"), \"w\") as f:\n                with open(label_file_path, \"r\") as original_label:\n                    f.write(original_label.read())\n\n    print(\"Bilanciamento completato: dataset bilanciato con augmentation.\")\n\nbalance_dataset(labels_path, images_path, output_labels_path, output_images_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T17:11:04.479445Z","iopub.execute_input":"2025-01-04T17:11:04.479854Z","iopub.status.idle":"2025-01-04T17:21:42.970359Z","shell.execute_reply.started":"2025-01-04T17:11:04.479815Z","shell.execute_reply":"2025-01-04T17:21:42.968300Z"}},"outputs":[{"name":"stdout","text":"sample negatives:  4646\nBilanciamento completato: dataset bilanciato con augmentation.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import os\nimport random\nimport shutil\n\ninput_labels_path = \"/kaggle/working/train_val_edit/labels\"\ninput_images_path = \"/kaggle/working/train_val_edit/images\"\n\ntrain_labels_path = \"/kaggle/working/dataset-soccer-last/train/labels\"\ntrain_images_path = \"/kaggle/working/dataset-soccer-last/train/images\"\nval_labels_path = \"/kaggle/working/dataset-soccer-last/val/labels\"\nval_images_path = \"/kaggle/working/dataset-soccer-last/val/images\"\n\nos.makedirs(train_labels_path, exist_ok=True)\nos.makedirs(train_images_path, exist_ok=True)\nos.makedirs(val_labels_path, exist_ok=True)\nos.makedirs(val_images_path, exist_ok=True)\n\ndef split_dataset(input_labels_path, input_images_path, train_labels_path, train_images_path, val_labels_path, val_images_path, split_ratio=0.8):\n\n    all_files = [f for f in os.listdir(input_labels_path) if f.endswith(\".txt\")]\n    random.shuffle(all_files)\n\n    split_index = int(len(all_files) * split_ratio)\n    train_files = all_files[:split_index]\n    val_files = all_files[split_index:]\n\n    for label_file in train_files:\n        shutil.copy(os.path.join(input_labels_path, label_file), os.path.join(train_labels_path, label_file))\n        image_file = label_file.replace(\".txt\", \".jpg\")\n        shutil.copy(os.path.join(input_images_path, image_file), os.path.join(train_images_path, image_file))\n\n    for label_file in val_files:\n        shutil.copy(os.path.join(input_labels_path, label_file), os.path.join(val_labels_path, label_file))\n        image_file = label_file.replace(\".txt\", \".jpg\")\n        shutil.copy(os.path.join(input_images_path, image_file), os.path.join(val_images_path, image_file))\n\n    print(f\"Dataset diviso: {len(train_files)} file per il train, {len(val_files)} file per il validation.\")\n\ndef create_zip(zip_path, base_dir):\n    shutil.make_archive(zip_path.replace(\".zip\", \"\"), 'zip', base_dir)\n    print(f\"File ZIP creato: {zip_path}\")\n\nsplit_dataset(input_labels_path, input_images_path, train_labels_path, train_images_path, val_labels_path, val_images_path)\n\nzip_output_path = \"/kaggle/working/dataset-soccer-last.zip\"\ncreate_zip(zip_output_path, \"/kaggle/working/dataset-soccer-last\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef count_files(directory, extension=None):\n    if not os.path.isdir(directory):\n        print(f\"La directory {directory} non esiste.\")\n        return 0\n\n    if extension:\n        # Filtra i file con l'estensione specificata\n        files = [f for f in os.listdir(directory) if f.endswith(extension)]\n    else:\n        # Conta tutti i file nella directory\n        files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n\n    return len(files)\n\ndirectory = \"/kaggle/working/dataset-soccer-last/train/images\"  \nextension = None  \n\nfile_count = count_files(directory, extension)\nprint(f\"Numero di file nella directory '{directory}': {file_count}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef count_files(directory, extension=None):\n    if not os.path.isdir(directory):\n        print(f\"La directory {directory} non esiste.\")\n        return 0\n\n    if extension:\n        files = [f for f in os.listdir(directory) if f.endswith(extension)]\n    else:\n        files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n    return len(files)\n\n# Percorsi delle cartelle\ntrain_labels_path = \"/kaggle/working/dataset-soccer-last/train/labels\"\ntrain_images_path = \"/kaggle/working/dataset-soccer-last/train/images\"\nval_labels_path = \"/kaggle/working/dataset-soccer-last/val/labels\"\nval_images_path = \"/kaggle/working/dataset-soccer-last/val/images\"\n\ndef validate_dataset():\n    print(\"Validazione del dataset:\")\n\n    train_labels_count = count_files(train_labels_path, \".txt\")\n    train_images_count = count_files(train_images_path, \".jpg\")\n    print(f\"Train set: {train_images_count} immagini, {train_labels_count} annotazioni.\")\n\n    val_labels_count = count_files(val_labels_path, \".txt\")\n    val_images_count = count_files(val_images_path, \".jpg\")\n    print(f\"Validation set: {val_images_count} immagini, {val_labels_count} annotazioni.\")\n\n    # Verifica\n    if train_images_count != train_labels_count:\n        print(\"[AVVISO] Il numero di immagini e annotazioni non corrisponde nel train set.\")\n    if val_images_count != val_labels_count:\n        print(\"[AVVISO] Il numero di immagini e annotazioni non corrisponde nel validation set.\")\n\n    total_files = train_images_count + val_images_count\n    print(f\"Totale file (train + validation): {total_files}\")\n\nvalidate_dataset()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}